<!DOCTYPE html>
<html lang="en" class="h-full">
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-1T6NPEB6QT"></script>
    <script data-cfasync="false">window.dataLayer = window.dataLayer || [];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());gtag('config', 'G-1T6NPEB6QT');</script>

    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="turbo-cache-control" content="no-cache" data-turbo-track="reload" data-track-token="3.4.1.748164555016">

    <!-- See retype.com -->
    <meta name="generator" content="Retype 3.4.1">

    <!-- Primary Meta Tags -->
    <title>LoRA - Pygmalion AI</title>
    <meta name="title" content="LoRA - Pygmalion AI">
    <meta name="description" content="Low-Rank Adaptation (LoRA) is a paradigm of natural language processing (NLP) that freezes the pre-trained model weights and injects trainable rank...">

    <!-- Canonical -->
    <link rel="canonical" href="https://docs.pygmalion.chat/pygmalion-extras/lora/">

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://docs.pygmalion.chat/pygmalion-extras/lora/">
    <meta property="og:title" content="LoRA - Pygmalion AI">
    <meta property="og:description" content="Low-Rank Adaptation (LoRA) is a paradigm of natural language processing (NLP) that freezes the pre-trained model weights and injects trainable rank...">
    <meta property="og:image" content="https://docs.pygmalion.chat/static/lora.png">

    <!-- Twitter -->
    <meta property="twitter:card" content="summary_large_image">
    <meta property="twitter:url" content="https://docs.pygmalion.chat/pygmalion-extras/lora/">
    <meta property="twitter:title" content="LoRA - Pygmalion AI">
    <meta property="twitter:description" content="Low-Rank Adaptation (LoRA) is a paradigm of natural language processing (NLP) that freezes the pre-trained model weights and injects trainable rank...">
    <meta property="twitter:image" content="https://docs.pygmalion.chat/static/lora.png">

    <script data-cfasync="false">(function () { var el = document.documentElement, m = localStorage.getItem("doc_theme"), wm = window.matchMedia; if (m === "dark" || (!m && wm && wm("(prefers-color-scheme: dark)").matches)) { el.classList.add("dark") } else { el.classList.remove("dark") } })();</script>

    <link href="../../static/icons/pyggy.svg" rel="icon">
    <link href="../../resources/css/retype.css?v=3.4.1.748164555016" rel="stylesheet">

    <script data-cfasync="false" src="../../resources/js/config.js?v=3.4.1.748164555016" data-turbo-eval="false" defer></script>
    <script data-cfasync="false" src="../../resources/js/retype.js?v=3.4.1" data-turbo-eval="false" defer></script>
    <script id="lunr-js" data-cfasync="false" src="../../resources/js/lunr.js?v=3.4.1.748164555016" data-turbo-eval="false" defer></script>
    <script id="prism-js" data-cfasync="false" src="../../resources/js/prism.js?v=3.4.1.748164555016" defer></script>
    <link href="../../resources/css/katex.css?v=3.4.1" rel="stylesheet">
    <script id="katex-js" data-cfasync="false" src="../../resources/js/katex.js?v=3.4.1" defer></script>
</head>
<body>
    <div id="docs-app" class="relative text-base antialiased text-gray-700 bg-white font-body dark:bg-dark-850 dark:text-dark-300">
        <div class="absolute bottom-0 left-0 bg-gray-100 dark:bg-dark-800" style="top: 5rem; right: 50%"></div>
    
        <header id="docs-site-header" class="sticky top-0 z-30 flex w-full h-16 bg-white border-b border-gray-200 md:h-20 dark:bg-dark-850 dark:border-dark-650">
            <div class="container relative flex items-center justify-between pr-6 grow md:justify-start">
                <!-- Mobile menu button skeleton -->
                <button v-cloak class="skeleton docs-mobile-menu-button flex items-center justify-center shrink-0 overflow-hidden dark:text-white focus:outline-none rounded-full w-10 h-10 ml-3.5 md:hidden"><svg xmlns="http://www.w3.org/2000/svg" class="mb-px shrink-0" width="24" height="24" viewBox="0 0 24 24" role="presentation" style="margin-bottom: 0px;"><g fill="currentColor"><path d="M2 4h20v2H2zM2 11h20v2H2zM2 18h20v2H2z"></path></g></svg></button>
                <div v-cloak id="docs-sidebar-toggle"></div>
        
                <!-- Logo -->
                <div class="flex items-center justify-between h-full py-2 md:w-75">
                    <div class="flex items-center px-2 md:px-6">
                        <a id="docs-site-logo" href="../../" class="flex items-center leading-snug text-2xl">
                            <span class="w-10 mr-2 grow-0 shrink-0 overflow-hidden">
                                <img class="max-h-10 dark:hidden md:inline-block" src="../../static/icons/pygmalion_fixed.svg">
                                <img class="max-h-10 hidden dark:inline-block" src="../../static/icons/pygmalion_fixed.svg">
                            </span>
                            <span class="dark:text-white font-semibold line-clamp-1 md:line-clamp-2">PygmalionAI</span>
                        </a><span class="hidden px-2 py-1 ml-4 text-sm font-semibold leading-none text-root-logo-label-text bg-root-logo-label-bg rounded-sm md:inline-block">Docs</span>
                    </div>
        
                    <span class="hidden h-8 border-r md:inline-block dark:border-dark-650"></span>
                </div>
        
                <div class="flex justify-between md:grow">
                    <!-- Top Nav -->
                    <nav class="hidden md:flex">
                        <ul class="flex flex-col mb-4 md:pl-16 md:mb-0 md:flex-row md:items-center">
                            <li class="mr-6">
                                <a class="py-2 md:mb-0 inline-flex items-center text-sm whitespace-nowrap transition-colors duration-200 ease-linear md:text-blue-500 dark:text-blue-400 hover:text-blue-800 dark:hover:text-blue-200" href="https://discord.gg/invite/pygmalionai">
                                    <img class="mb-px mr-1" width="18" alt="" src="../../static/icons/discord-logo.svg">
                                    <span>Discord</span>
                                </a>
                            </li>
                            <li class="mr-6">
                                <a class="py-2 md:mb-0 inline-flex items-center text-sm whitespace-nowrap transition-colors duration-200 ease-linear md:text-blue-500 dark:text-blue-400 hover:text-blue-800 dark:hover:text-blue-200" href="https://huggingface.co/PygmalionAI">
                                    <img class="mb-px mr-1" width="18" alt="" src="../../static/icons/hf-logo.svg">
                                    <span>HuggingFace</span>
                                </a>
                            </li>
                            <li class="mr-6">
                                <a class="py-2 md:mb-0 inline-flex items-center text-sm whitespace-nowrap transition-colors duration-200 ease-linear md:text-blue-500 dark:text-blue-400 hover:text-blue-800 dark:hover:text-blue-200" href="https://github.com/PygmalionAI">
                                    <svg xmlns="http://www.w3.org/2000/svg" class="mb-px mr-1" width="18" height="18" viewBox="0 0 24 24" role="presentation">
                                        <g fill="currentColor">
                                            <symbol id="mark-github-icon-symbol" viewBox="0 0 16 16"><path d="M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z"/></symbol><use xlink:href="#mark-github-icon-symbol" />
                                        </g>
                                    </svg>
                                    <span>GitHub</span>
                                </a>
                            </li>
        
                        </ul>
                    </nav>
        
                    <!-- Header Right Skeleton -->
                    <div v-cloak class="flex justify-end grow skeleton">
        
                        <!-- Search input mock -->
                        <div class="relative hidden w-40 lg:block lg:max-w-sm lg:ml-auto">
                            <div class="absolute flex items-center justify-center h-full pl-3 dark:text-dark-300">
                                <svg xmlns="http://www.w3.org/2000/svg" class="icon-base" width="16" height="16" viewBox="0 0 24 24" aria-labelledby="icon" role="presentation" style="margin-bottom: 1px;"><g fill="currentColor" ><path d="M21.71 20.29l-3.68-3.68A8.963 8.963 0 0020 11c0-4.96-4.04-9-9-9s-9 4.04-9 9 4.04 9 9 9c2.12 0 4.07-.74 5.61-1.97l3.68 3.68c.2.19.45.29.71.29s.51-.1.71-.29c.39-.39.39-1.03 0-1.42zM4 11c0-3.86 3.14-7 7-7s7 3.14 7 7c0 1.92-.78 3.66-2.04 4.93-.01.01-.02.01-.02.01-.01.01-.01.01-.01.02A6.98 6.98 0 0111 18c-3.86 0-7-3.14-7-7z" ></path></g></svg>
                            </div>
                            <input class="w-full h-10 placeholder-gray-400 transition-colors duration-200 ease-in bg-gray-200 border border-transparent rounded md:text-sm hover:bg-white hover:border-gray-300 focus:outline-none focus:bg-white focus:border-gray-500 dark:bg-dark-600 dark:border-dark-600 dark:placeholder-dark-400" style="padding: 0.625rem 0.75rem 0.625rem 2rem" type="text" placeholder="Search">
                        </div>
        
                        <!-- Mobile search button -->
                        <div class="flex items-center justify-center w-10 h-10 lg:hidden">
                            <svg xmlns="http://www.w3.org/2000/svg" class="shrink-0 icon-base" width="20" height="20" viewBox="0 0 24 24" aria-labelledby="icon" role="presentation" style="margin-bottom: 0px;"><g fill="currentColor" ><path d="M21.71 20.29l-3.68-3.68A8.963 8.963 0 0020 11c0-4.96-4.04-9-9-9s-9 4.04-9 9 4.04 9 9 9c2.12 0 4.07-.74 5.61-1.97l3.68 3.68c.2.19.45.29.71.29s.51-.1.71-.29c.39-.39.39-1.03 0-1.42zM4 11c0-3.86 3.14-7 7-7s7 3.14 7 7c0 1.92-.78 3.66-2.04 4.93-.01.01-.02.01-.02.01-.01.01-.01.01-.01.02A6.98 6.98 0 0111 18c-3.86 0-7-3.14-7-7z" ></path></g></svg>
                        </div>
        
                        <!-- Dark mode switch placeholder -->
                        <div class="w-10 h-10 lg:ml-2"></div>
        
                        <!-- History button -->
                        <div class="flex items-center justify-center w-10 h-10" style="margin-right: -0.625rem;">
                            <svg xmlns="http://www.w3.org/2000/svg" class="shrink-0 icon-base" width="22" height="22" viewBox="0 0 24 24" aria-labelledby="icon" role="presentation" style="margin-bottom: 0px;"><g fill="currentColor" ><g ><path d="M12.01 6.01c-.55 0-1 .45-1 1V12a1 1 0 00.4.8l3 2.22a.985.985 0 001.39-.2.996.996 0 00-.21-1.4l-2.6-1.92V7.01c.02-.55-.43-1-.98-1z"></path><path d="M12.01 1.91c-5.33 0-9.69 4.16-10.05 9.4l-.29-.26a.997.997 0 10-1.34 1.48l1.97 1.79c.19.17.43.26.67.26s.48-.09.67-.26l1.97-1.79a.997.997 0 10-1.34-1.48l-.31.28c.34-4.14 3.82-7.41 8.05-7.41 4.46 0 8.08 3.63 8.08 8.09s-3.63 8.08-8.08 8.08c-2.18 0-4.22-.85-5.75-2.4a.996.996 0 10-1.42 1.4 10.02 10.02 0 007.17 2.99c5.56 0 10.08-4.52 10.08-10.08.01-5.56-4.52-10.09-10.08-10.09z"></path></g></g></svg>
                        </div>
                    </div>
        
                    <div v-cloak class="flex justify-end grow">
                        <div id="docs-mobile-search-button"></div>
                        <doc-search-desktop></doc-search-desktop>
        
                        <doc-theme-switch class="lg:ml-2"></doc-theme-switch>
                        <doc-history></doc-history>
                    </div>
                </div>
            </div>
        </header>
    
        <div class="container relative flex bg-white">
            <!-- Sidebar Skeleton -->
            <div v-cloak class="fixed flex flex-col shrink-0 duration-300 ease-in-out bg-gray-100 border-gray-200 sidebar top-20 w-75 border-r h-screen md:sticky transition-transform skeleton dark:bg-dark-800 dark:border-dark-650">
            
                <!-- Render this div, if config.showSidebarFilter is `true` -->
                <div class="flex items-center h-16 px-6">
                    <input class="w-full h-8 px-3 py-2 transition-colors duration-200 ease-linear bg-white border border-gray-200 rounded shadow-none text-sm focus:outline-none focus:border-gray-600 dark:bg-dark-600 dark:border-dark-600" type="text" placeholder="Filter">
                </div>
            
                <div class="pl-6 mt-1 mb-4">
                    <div class="w-32 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
                    <div class="w-48 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
                    <div class="w-40 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
                    <div class="w-32 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
                    <div class="w-48 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
                    <div class="w-40 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
                </div>
            
                <div class="shrink-0 mt-auto bg-transparent dark:border-dark-650">
                    <a class="flex items-center justify-center flex-nowrap h-16 text-gray-400 dark:text-dark-400 hover:text-gray-700 dark:hover:text-dark-300 transition-colors duration-150 ease-in docs-powered-by" target="_blank" href="https://retype.com/" rel="noopener">
                        <span class="text-xs whitespace-nowrap">Powered by</span>
                        <svg xmlns="http://www.w3.org/2000/svg" class="ml-2" fill="currentColor" width="96" height="20" overflow="visible"><path d="M0 0v20h13.59V0H0zm11.15 17.54H2.44V2.46h8.71v15.08zM15.8 20h2.44V4.67L15.8 2.22zM20.45 6.89V20h2.44V9.34z"/><g><path d="M40.16 8.44c0 1.49-.59 2.45-1.75 2.88l2.34 3.32h-2.53l-2.04-2.96h-1.43v2.96h-2.06V5.36h3.5c1.43 0 2.46.24 3.07.73s.9 1.27.9 2.35zm-2.48 1.1c.26-.23.38-.59.38-1.09 0-.5-.13-.84-.4-1.03s-.73-.28-1.39-.28h-1.54v2.75h1.5c.72 0 1.2-.12 1.45-.35zM51.56 5.36V7.2h-4.59v1.91h4.13v1.76h-4.13v1.92h4.74v1.83h-6.79V5.36h6.64zM60.09 7.15v7.48h-2.06V7.15h-2.61V5.36h7.28v1.79h-2.61zM70.81 14.64h-2.06v-3.66l-3.19-5.61h2.23l1.99 3.45 1.99-3.45H74l-3.19 5.61v3.66zM83.99 6.19c.65.55.97 1.4.97 2.55s-.33 1.98-1 2.51-1.68.8-3.04.8h-1.23v2.59h-2.06V5.36h3.26c1.42 0 2.45.28 3.1.83zm-1.51 3.65c.25-.28.37-.69.37-1.22s-.16-.92-.48-1.14c-.32-.23-.82-.34-1.5-.34H79.7v3.12h1.38c.68 0 1.15-.14 1.4-.42zM95.85 5.36V7.2h-4.59v1.91h4.13v1.76h-4.13v1.92H96v1.83h-6.79V5.36h6.64z"/></g></svg>
                    </a>
                </div>
            </div>
            
            <!-- Sidebar component -->
            <doc-sidebar v-cloak>
                <template #sidebar-footer>
                    <div class="shrink-0 mt-auto border-t md:bg-transparent md:border-none dark:border-dark-650">
            
                        <div class="py-3 px-6 md:hidden border-b dark:border-dark-650">
                            <nav>
                                <ul class="flex flex-wrap justify-center items-center">
                                    <li class="mr-6">
                                        <a class="block py-1 inline-flex items-center text-sm whitespace-nowrap transition-colors duration-200 ease-linear md:text-blue-500 dark:text-blue-400 hover:text-blue-800 dark:hover:text-blue-200" href="https://discord.gg/invite/pygmalionai">
                                            <img class="mb-px mr-1" width="18" alt="" src="../../static/icons/discord-logo.svg">
                                            <span>Discord</span>
                                        </a>
                                    </li>
                                    <li class="mr-6">
                                        <a class="block py-1 inline-flex items-center text-sm whitespace-nowrap transition-colors duration-200 ease-linear md:text-blue-500 dark:text-blue-400 hover:text-blue-800 dark:hover:text-blue-200" href="https://huggingface.co/PygmalionAI">
                                            <img class="mb-px mr-1" width="18" alt="" src="../../static/icons/hf-logo.svg">
                                            <span>HuggingFace</span>
                                        </a>
                                    </li>
                                    <li class="mr-6">
                                        <a class="block py-1 inline-flex items-center text-sm whitespace-nowrap transition-colors duration-200 ease-linear md:text-blue-500 dark:text-blue-400 hover:text-blue-800 dark:hover:text-blue-200" href="https://github.com/PygmalionAI">
                                            <svg xmlns="http://www.w3.org/2000/svg" class="mb-px mr-1" width="18" height="18" viewBox="0 0 24 24" role="presentation">
                                                <g fill="currentColor">
                                                    <symbol id="mark-github-icon-symbol" viewBox="0 0 16 16"><path d="M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z"/></symbol><use xlink:href="#mark-github-icon-symbol" />
                                                </g>
                                            </svg>
                                            <span>GitHub</span>
                                        </a>
                                    </li>
            
                                </ul>
                            </nav>
                        </div>
            
                        <a class="flex items-center justify-center flex-nowrap h-16 text-gray-400 dark:text-dark-400 hover:text-gray-700 dark:hover:text-dark-300 transition-colors duration-150 ease-in docs-powered-by" target="_blank" href="https://retype.com/" rel="noopener">
                            <span class="text-xs whitespace-nowrap">Powered by</span>
                            <svg xmlns="http://www.w3.org/2000/svg" class="ml-2" fill="currentColor" width="96" height="20" overflow="visible"><path d="M0 0v20h13.59V0H0zm11.15 17.54H2.44V2.46h8.71v15.08zM15.8 20h2.44V4.67L15.8 2.22zM20.45 6.89V20h2.44V9.34z"/><g><path d="M40.16 8.44c0 1.49-.59 2.45-1.75 2.88l2.34 3.32h-2.53l-2.04-2.96h-1.43v2.96h-2.06V5.36h3.5c1.43 0 2.46.24 3.07.73s.9 1.27.9 2.35zm-2.48 1.1c.26-.23.38-.59.38-1.09 0-.5-.13-.84-.4-1.03s-.73-.28-1.39-.28h-1.54v2.75h1.5c.72 0 1.2-.12 1.45-.35zM51.56 5.36V7.2h-4.59v1.91h4.13v1.76h-4.13v1.92h4.74v1.83h-6.79V5.36h6.64zM60.09 7.15v7.48h-2.06V7.15h-2.61V5.36h7.28v1.79h-2.61zM70.81 14.64h-2.06v-3.66l-3.19-5.61h2.23l1.99 3.45 1.99-3.45H74l-3.19 5.61v3.66zM83.99 6.19c.65.55.97 1.4.97 2.55s-.33 1.98-1 2.51-1.68.8-3.04.8h-1.23v2.59h-2.06V5.36h3.26c1.42 0 2.45.28 3.1.83zm-1.51 3.65c.25-.28.37-.69.37-1.22s-.16-.92-.48-1.14c-.32-.23-.82-.34-1.5-.34H79.7v3.12h1.38c.68 0 1.15-.14 1.4-.42zM95.85 5.36V7.2h-4.59v1.91h4.13v1.76h-4.13v1.92H96v1.83h-6.79V5.36h6.64z"/></g></svg>
                        </a>
                    </div>
                </template>
            </doc-sidebar>
    
            <div class="grow min-w-0 dark:bg-dark-850">
                <!-- Render "toolbar" template here on api pages --><!-- Render page content -->
                <div class="flex">
                    <div class="min-w-0 p-4 grow md:px-16">
                        <main class="relative pb-12 lg:pt-2">
                            <div class="docs-markdown" id="docs-content">
                                <!-- Rendered if sidebar right is enabled -->
                                <div id="docs-sidebar-right-toggle"></div>
                                <!-- Page content  -->
<doc-anchor-target id="lora" class="break-words">
    <h1>
        <doc-anchor-trigger class="header-anchor-trigger" to="#lora">#</doc-anchor-trigger>
        <span>LoRA</span>
    </h1>
</doc-anchor-target>
<p>Low-Rank Adaptation (LoRA) is a paradigm of natural language processing (NLP) that freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture. Compared to GPT-175B fine-tuned with <a href="https://arxiv.org/abs/1412.6980">Adam</a>, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirements by 3 times. Refer to the paper below:</p>
<p>
<div class="mb-6 px-5 py-4 flex justify-between relative border border-gray-300 hover:border-gray-400 dark:border-dark-650 dark:hover:border-dark-450 rounded transition-colors duration-150">
    <div class="flex items-center text-blue-500 dark:text-blue-400">
        <span class="inline-block mb-px">
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" role="presentation" fill="currentColor"><path d="M20.322.75h1.176a1.75 1.75 0 0 1 1.75 1.749v1.177a10.75 10.75 0 0 1-2.925 7.374l-1.228 1.304a23.699 23.699 0 0 1-1.596 1.542v5.038c0 .615-.323 1.184-.85 1.5l-4.514 2.709a.75.75 0 0 1-1.12-.488l-.963-4.572a1.305 1.305 0 0 1-.14-.129L8.04 15.96l-1.994-1.873a1.305 1.305 0 0 1-.129-.14l-4.571-.963a.75.75 0 0 1-.49-1.12l2.71-4.514c.316-.527.885-.85 1.5-.85h5.037a23.668 23.668 0 0 1 1.542-1.594l1.304-1.23A10.753 10.753 0 0 1 20.321.75Zm-6.344 4.018v-.001l-1.304 1.23a22.275 22.275 0 0 0-3.255 3.851l-2.193 3.29 1.859 1.744a.545.545 0 0 1 .034.034l1.743 1.858 3.288-2.192a22.263 22.263 0 0 0 3.854-3.257l1.228-1.303a9.251 9.251 0 0 0 2.517-6.346V2.5a.25.25 0 0 0-.25-.25h-1.177a9.252 9.252 0 0 0-6.344 2.518ZM6.5 21c-1.209 1.209-3.901 1.445-4.743 1.49a.236.236 0 0 1-.18-.067.236.236 0 0 1-.067-.18c.045-.842.281-3.534 1.49-4.743.9-.9 2.6-.9 3.5 0 .9.9.9 2.6 0 3.5Zm-.592-8.588L8.17 9.017c.23-.346.47-.685.717-1.017H5.066a.25.25 0 0 0-.214.121l-2.167 3.612ZM16 15.112c-.333.248-.672.487-1.018.718l-3.393 2.262.678 3.223 3.612-2.167a.25.25 0 0 0 .121-.214ZM17.5 8a1.5 1.5 0 1 1-3.001-.001A1.5 1.5 0 0 1 17.5 8Z"/></svg>
        </span>
        <span class="inline-block font-medium ml-2">LoRA: Low-Rank Adaptation of Large Language Models</span>
    </div>
    <div class="flex items-center text-xs text-gray-400 dark:text-dark-400">
        <span>https://arxiv.org/abs/2106.09685</span>
    </div>
    <a href="https://arxiv.org/abs/2106.09685" class="absolute block inset-0"></a>
</div>
</p>
<doc-anchor-target id="low-rank-adaptation">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#low-rank-adaptation">#</doc-anchor-trigger>
        <span>Low-Rank Adaptation</span>
    </h2>
</doc-anchor-target>
<p>LoRAs are preferred when the traditional fine-tuning methods prove to be too computationally expensive. As demonstrated by Edward Hu et al., training a LoRA is at least 3 times less resource intensive as a native fine-tune. This can be further reduced with INT4 LoRA tuning - made possible by <a href="https://github.com/IST-DASLab/gptq">GPTQ</a>.</p>
<p>Some of the key advantages LoRAs possess are:</p>
<ul>
<li>A pre-trained model can be shared and used to build many small LoRA modules for different downstream tasks. We can freeze the shared model and efficiently switch tasks by replacing the matrices <code v-pre>A</code> and <code v-pre>B</code> as demonstrated below, reducing the storage requirement and task-switching overhead significantly.</li>
</ul>
<p><figure class="content-left">
    <img src="../../static/lora.png" alt="Figure 1">
    <figcaption class="caption caption-float">Figure 1</figcaption>
</figure>
</p>
<ul>
<li><p>LoRA makes training more efficient and lowers the hardware barrier to entry by up to 3 times (over 10 times if used in conjunction with GPTQ) when using adaptive optimizers since we don&#x27;t need to calculate the gradients or maintain the optimizer states for most parameters. Instead we only optimize the injected, <em>much smaller</em> low-rank matrices.</p>
</li>
<li><p>The simple linear design allows us to merge the trainable matrices with the frozen weights when deployed, <em><strong>instroducing no inference latency</strong></em> compared to a fully fine-tuned model, by construction.</p>
</li>
<li><p>LoRA is orthogonal to many prior methods and can be combined with many of them, such as prefix-tuning and prompt-tuning.</p>
</li>
</ul>
<hr>
<p>LoRAs were originally designed for LLMs, but they can be used for other objectives, such as diffusion models. The docs here will focus on the language modeling objective of LoRAs. We try to maximize the likelihood of a predicted word given the previous words in a sentence. Imagine we&#x27;re given a GPT model <span class="math">\Rho_\Phi(y|x)</span> which is parametrized by <span class="math">\Phi</span>. For example, <span class="math">\Rho_\Phi(y|x)</span> can be a generic model based on the <a href="https://github.com/huggingface/transformers">Transformers</a> architecure. Now imagine we will need to adapt this large model to different downstream tasks, such as summarization, machine reading comprehension, and natural language to SQL (NL2SQL). We&#x27;ll represent each downstream task by a training dataset of context-target pairs: <span class="math">\large {Z=\text{\textbraceleft}(x_i,y_i)\text{\textbraceright}}_{i = 1,...N}</span> where both <span class="math">x_i</span> and <span class="math">y_i</span> are sequences of tokens. For example, in summarization, <span class="math">x_i</span> is the content of the article and <span class="math">y_i</span> is its summary. During full fine-tuning, the model is initialized to pre-trained weights <span class="math">\Phi_0</span> and updated to <span class="math">\Phi_0 + \Delta\Phi</span> by repeatedly following the gradient to maximize the conditional language modeling objective:</p>
<div class="math"><pre>\huge{\overset{max}\varPhi \sum_{(x,y)\in\Z}\overset{|y|}\sum_{t=1}\log\Large{(P_\Phi(y_t|x,y&lt;_t))}}</pre></div>
<p>One of the main disadvantages for full fine-tuning is that for <em>each</em> downstream task, we learn a <em>different</em> set of parameters <span class="math">\Delta\Phi</span>, whose dimensions <span class="math">|\Delta\Phi|</span> equals <span class="math">|\Phi_0|</span>. If the model is large (such as GPT-3 with 175 Billion parameters (<span class="math">|\Phi_0|</span>)), storing and running independent instances of the model for different tasks would be extremely difficult.</p>
<p>LoRAs aim to solve this exact problem. The task-specific parameter increment <span class="math">\Delta\Phi = \Delta\Phi(\Theta)</span> is encoded by a much smaller-sized set of parameters <span class="math">\Theta</span> with <span class="math">|\Theta| \ll |\Phi_0|</span>. The task of finding <span class="math">\Delta\Phi</span> then becomes optimizing over <span class="math">\Theta</span>:</p>
<div class="math"><pre>\huge{\overset{max}\varPhi \sum_{(x,y)\in\Z}\overset{|y|}\sum_{t=1}\log\Large{(p_{\Phi _0}+\Delta\Phi(\Theta)(y_t|x,y&lt;_t))}}</pre></div>
<p>LoRA uses a low-rank representation to encode <span class="math">\Delta\Phi</span> that&#x27;s both compute and memory efficient. When the model is GPT-3 175B, the number of trainable parameters <span class="math">|\Theta|</span> can be as small as 0.01% of <span class="math">|\Theta_0|</span>!</p>
<doc-anchor-target id="how-does-it-work">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#how-does-it-work">#</doc-anchor-trigger>
        <span>How does it work?</span>
    </h3>
</doc-anchor-target>
<p>A neural network contains many dense layers which perform matrix multiplication. The weight matrices in these layers ususally have full-rank. When adapting a specific task, GPT models have a low &quot;instrisic dimension&quot; and can still learn efficiently despite a random projection to a smaller subspace. Inspired by this, Edward Hu et al. hypothesized that the updates to the weights also have a low &quot;instrinsic rank&quot; during adaptation. For a pre-trained weight matrix <span class="math">W_0 \in \R^{d x k}</span>, we constrain its update by representing the latter with a low-rank decomposition <span class="math">W_0 + \Delta W = W_0 + B A</span> are multiplied with the same input, and their respective output vectors are summed coordinate-wise. For <span class="math">h = W_0x</span>, our modified forward pass yields:</p>
<div class="math"><pre>\Large{h = W_0 x + \Delta W_x = W_0 x + B A_x}</pre></div>
<p>Edward Hu et al. illustrate the reparametrization in Figure 1. They used a random Gaussian initialization for <span class="math">A</span> and zero for <span class="math">B</span>, so <span class="math">\Delta W = B A</span> is zero at the beginning of the training. We then scale <span class="math">\Delta W_x</span> by <span class="math">\large{\frac{\alpha}{r}}</span>, where <span class="math">\alpha</span> is a constant in <span class="math">r</span>. When optimizing with <a href="https://arxiv.org/abs/1412.6980">Adam</a>, tuning <span class="math">a</span> is roughly the same as tuning the learning rate if we scale the initialization appropriately. As a result, we simply set <span class="math">a</span> to the first <span class="math">r</span> we try and do not tune it. This scaling helps to reduce the need to retune hyperparameters when we vary <span class="math">r</span>. This method introduces <strong>no additional inference latency</strong>. When deployed in production, we can explicitly compute and store <span class="math">W = W_0 + B A</span> and perform inference as usual. Note that both <span class="math">W_0</span> and <span class="math">B A</span> are in <span class="math">\R^{d x k}</span>. When we need to switch to another downstream ask (switch LoRA adapters), we can recover <span class="math">W_0</span> by subtracting <span class="math">B A</span> and then adding a different <span class="math">B' A'</span>, a quick operation with very little memory overhead.</p>
<doc-anchor-target id="what-are-the-benefits-of-lora">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#what-are-the-benefits-of-lora">#</doc-anchor-trigger>
        <span>What are the benefits of LoRA?</span>
    </h3>
</doc-anchor-target>
<p>The most significant benefit comes from the reduction in memory and storage usage. For a large model, such as GPT-J, trained with Adam, we reduce that VRAM usage by up to 2/3 if <span class="math">r \ll d_{model}</span> as we do not need to store the optimizer states for the frozen params. On GPT-3 175B, we can reduce the VRAM consumption during training from 1.2TB to 350GB. With <span class="math">r = 4</span> and only the query and value projection matrices being adapted, the checkpoint size is reduced by roughly 10,000x (from 350GB to 35MB). This allows us to train with significantly fewer GPUs and avoid I/O bottlenecks. Another benefit is that we can switch between tasks while deployed at a much lower cost by only swapping the LoRA weights as opposed to all the parameters. This allows for creation of many customized models that can be swapped in and out on the fly.</p>
<p>LoRA also has its limitations. For example, it&#x27;s not straightforward to batch inputs to different tasks with different <span class="math">A</span> and <span class="math">B</span> in a single forward pass, if one chooses to absorb <span class="math">A</span> and <span class="math">B</span> into <span class="math">W</span> to eliminate additional latency. Though it&#x27;s possible to not merge the weights and dynamically choose the LoRA modules to use for samples in a batch for scenarios where latency is not very important.</p>
<doc-anchor-target id="how-do-i-train-a-lora">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#how-do-i-train-a-lora">#</doc-anchor-trigger>
        <span>How do I train a LoRA?</span>
    </h2>
</doc-anchor-target>
<p>The <a href="https://github.com/stochasticai/xturing">xTuring</a> repository contains code for both LoRA and full fine-tuning of LLMs such as LLaMA, GPT-J, and more. The current GPT-J (Pygmalion 6B) training code is only compatible with Alpaca and GPT4All dataset formats, so it will likely be useless for most users. There&#x27;s currently work being done on creating INT4 LoRA training code for GPT-J, so please be patient and keep an eye out for any updates here.</p>
<doc-anchor-target id="how-do-i-load-a-lora">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#how-do-i-load-a-lora">#</doc-anchor-trigger>
        <span>How do I load a LoRA?</span>
    </h2>
</doc-anchor-target>
<p>First, choose what LoRA you want to use. We have a list in the <a href="https://discord.com/invite/pygmalionai">Discord Server</a>. The steps will differ for both Kobold and Oobabooga (TextGen WebUI).</p>
<doc-anchor-target id="oobabooga-textgen-webui">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#oobabooga-textgen-webui">#</doc-anchor-trigger>
        <span>Oobabooga (TextGen WebUI)</span>
    </h3>
</doc-anchor-target>
<p>Currently, the easiest way to load a LoRA is via Oobabooga. Open a PowerShell/Terminal instance (press Shift + Righ-Click inside the folder and not on a file, then select &quot;Open in PowerShell), and run this command:</p>
<div class="codeblock-wrapper"><doc-codeblock>
<pre class="language-bash"><code v-pre class="language-bash">python3 download-model.py tloen/alpaca-lora-7b</code></pre>
</doc-codeblock></div>
<blockquote>
<p>Replace <code v-pre>tloen/alpaca-lora-7b</code> with the name of the LoRA repo you want. For example, if the link in the Discord post for the LoRA says <code v-pre>https://huggingface.co/nomic-ai/gpt4all-lora</code>, then you&#x27;ll have to put <code v-pre>nomic-ai/gpt4all-lora</code> in there instead.</p>
</blockquote>
<p>Then you can launch Oobabooga normally and then select the LoRA from the &quot;Parameters&quot; tab.</p>
<doc-anchor-target id="koboldai">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#koboldai">#</doc-anchor-trigger>
        <span>KoboldAI</span>
    </h3>
</doc-anchor-target>
<p>To use a LoRA with KoboldAI, you will need to merge the LoRA with the base model first. First, grab the LoRA you want from the <a href="https://discord.com/invite/pygmalionai">Discord Server</a>. You can use <a href="https://docs.pygmalion.chat/tools/git"><code v-pre>git lfs</code></a> to download the LoRA. Place the downloaded LoRA (if you&#x27;re on Windows, the default download path is <code v-pre>C:\Users\username\</code>) inside KoboldAI&#x27;s <code v-pre>models</code> folder.</p>
<p>Then grab the merge script from <a href="https://github.com/AlpinDale/lora-merge/blob/main/merge.py">here</a> and place it in the <code v-pre>models</code> folder of your KoboldAI directory.</p>
<p>Assuming your Pygmalion model folder is named &quot;pygmalion-7b&quot;, open a PowerShell/Terminal instance inside the <code v-pre>models</code> folder (on Windows, press Shift + Right-Click inside the folder and choose &quot;Open in Powershell&quot;), and type this command in:</p>
<div class="codeblock-wrapper"><doc-codeblock>
<pre class="language-bash"><code v-pre class="language-bash">python3 merge.py --base_model pygmalion-7b --lora alpaca-lora --output pygmalion-7b-alpaca-lora</code></pre>
</doc-codeblock></div>
<blockquote>
<p>Replace <code v-pre>alpaca-lora</code> with the name of the LoRA folder!</p>
</blockquote>
<p>Now, you can load the new model from the <code v-pre>pygmalion-7b-alpaca-lora</code> folder, or whatever else you decided to name it.</p>
<doc-anchor-target id="koboldcpp">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#koboldcpp">#</doc-anchor-trigger>
        <span>koboldcpp</span>
    </h3>
</doc-anchor-target>
<p>Using LoRAs with <a href="https://docs.pygmalion.chat/local-installation-(cpu)/pygcpp/">koboldcpp</a> requires converting the LoRA to the ggml format first. To do this, grab the LoRA you want from the <a href="https://discord.com/invite/pygmalionai">Discord Server</a>. You can use <a href="https://docs.pygmalion.chat/tools/git"><code v-pre>git lfs</code></a> to download the LoRA.</p>
<p>Download <a href="https://github.com/ggerganov/llama.cpp/blob/master/convert-lora-to-ggml.py">the lora-to-ggml script</a> from the llama.cpp repo, and place as the LoRA folder, but not inside it.</p>
<p>Then you can convert the LoRA using this command:</p>
<div class="codeblock-wrapper"><doc-codeblock>
<pre class="language-bash"><code v-pre class="language-bash">python3 convert-lora-to-ggml.py alpaca-lora alpaca-lora-ggml</code></pre>
</doc-codeblock></div>
<blockquote>
<p>Replace <code v-pre>alpaca-lora</code> with the name of the LoRA folder you&#x27;ve downloaded. Same for the output folder.</p>
</blockquote>
<p>Place the converted folder in a path you can easily remember, preferably inside the koboldcpp folder (or where the .exe file is for windows).</p>
<p>Then you can run this command:</p>
<div class="codeblock-wrapper"><doc-codeblock>
<pre class="language-bash"><code v-pre class="language-bash">python koboldcpp.py --lora alpaca-lora-ggml --nommap --unbantokens</code></pre>
</doc-codeblock></div>
<blockquote>
<p>Replace the <code v-pre>alpaca-lora-ggml</code> with the LoRA folder name. You can add any other arguments you want aside from these.</p>
</blockquote>
<doc-anchor-target id="koboldai-4bit">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#koboldai-4bit">#</doc-anchor-trigger>
        <span>KoboldAI 4bit</span>
    </h3>
</doc-anchor-target>
<p>There&#x27;s no way to directly use a LoRA in Kobold, not even in the 4bit branch. You will need to merge it with the <em>base</em> model, and then convert it to 4bit. The conversion process for 7B takes about 9GB of VRAM so it might be impossible for most users. If you still want to attempt it, follow the steps for KoboldAI until you get a merged model. Then use the <a href="https://github.com/0cc4m/GPTQ-for-LLaMa">GPTQ-for-LLaMA</a> repo to convert the model to 4bit GPTQ format. There are guides in the repo on how to do that.</p>

                                
                                <!-- Required only on API pages -->
                                <doc-toolbar-member-filter-no-results></doc-toolbar-member-filter-no-results>
                            </div>
                            <footer class="clear-both">
                                <div class="flex flex-wrap items-center justify-between mt-14">
                                    <a class="my-2.5 inline-flex items-center text-sm whitespace-nowrap text-blue-500 dark:text-blue-400 hover:text-blue-700 hover:underline" href="https://github.com/PygmalionAI/pygmalion-docs/blob/main/src/Pygmalion Extras/LoRA.md" target="_blank" rel="noopener">
                                        <svg class="mr-1.5" xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="currentColor" overflow="visible"><path d="M20 12c-.55 0-1 .45-1 1v7c0 .55-.45 1-1 1H4c-.55 0-1-.45-1-1V6c0-.55.45-1 1-1h7c.55 0 1-.45 1-1s-.45-1-1-1H4C2.35 3 1 4.35 1 6v14c0 1.65 1.35 3 3 3h14c1.65 0 3-1.35 3-3v-7c0-.55-.45-1-1-1z" /><path d="M22.21 1.79c-1.18-1.18-3.24-1.18-4.41 0l-9.5 9.5c-.13.13-.22.29-.26.46l-1 4c-.08.34.01.7.26.95.18.2.44.3.7.3.08 0 .16-.01.24-.03l4-1c.18-.04.34-.13.46-.26l9.5-9.5c1.22-1.22 1.22-3.2.01-4.42zm-1.42 3l-9.3 9.3-2.11.53.53-2.11 9.3-9.3c.42-.42 1.16-.42 1.59 0 .43.43.43 1.15-.01 1.58z" /><path fill="none" d="M0 0h24v24H0z" /></svg>
                                        <span>Edit this page</span>
                                    </a>
                                </div>
                            
                                <nav class="flex mt-14">
                                    <div class="w-1/2">
                                        <a class="px-5 py-4 h-full flex items-center break-normal font-medium text-blue-500 dark:text-blue-400 border border-gray-300 hover:border-gray-400 dark:border-dark-650 dark:hover:border-dark-450 rounded-l-lg transition-colors duration-150 relative hover:z-5" href="../../pygmalion-extras/soft-prompt/">
                                            <svg xmlns="http://www.w3.org/2000/svg" class="mr-3" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" overflow="visible"><path d="M19 11H7.41l5.29-5.29a.996.996 0 10-1.41-1.41l-7 7a1 1 0 000 1.42l7 7a1.024 1.024 0 001.42-.01.996.996 0 000-1.41L7.41 13H19c.55 0 1-.45 1-1s-.45-1-1-1z" /><path fill="none" d="M0 0h24v24H0z" /></svg>
                                            <span>
                                                <span class="block text-xs font-normal text-gray-400 dark:text-dark-400">Previous</span>
                                                <span class="block mt-1">Soft Prompts</span>
                                            </span>
                                        </a>
                                    </div>
                            
                                    <div class="w-1/2">
                                        <a class="px-5 py-4 -mx-px h-full flex items-center justify-end break-normal font-medium text-blue-500 dark:text-blue-400 border border-gray-300 hover:border-gray-400 dark:border-dark-650 dark:hover:border-dark-450 rounded-r-lg transition-colors duration-150 relative hover:z-5" href="../../tools/git/">
                                            <span>
                                                <span class="block text-xs font-normal text-right text-gray-400 dark:text-dark-400">Next</span>
                                                <span class="block mt-1">Git</span>
                                            </span>
                                            <svg xmlns="http://www.w3.org/2000/svg" class="ml-3" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" overflow="visible"><path d="M19.92 12.38a1 1 0 00-.22-1.09l-7-7a.996.996 0 10-1.41 1.41l5.3 5.3H5c-.55 0-1 .45-1 1s.45 1 1 1h11.59l-5.29 5.29a.996.996 0 000 1.41c.19.2.44.3.7.3s.51-.1.71-.29l7-7c.09-.09.16-.21.21-.33z" /><path fill="none" d="M0 0h24v24H0z" /></svg>
                                        </a>
                                    </div>
                                </nav>
                            </footer>
                        </main>
                
                        <div class="border-t dark:border-dark-650 pt-6 mb-8">
                            <footer class="flex flex-wrap items-center justify-between">
                                <div>
                                    <ul class="flex flex-wrap items-center text-sm">
                                    </ul>
                                </div>
                                <div class="docs-copyright py-2 text-gray-500 dark:text-dark-350 text-sm leading-relaxed"><p>© Copyleft 2023. All rights reserved.</p></div>
                            </footer>
                        </div>
                    </div>
                
                    <!-- Rendered if sidebar right is enabled -->
                    <!-- Sidebar right skeleton-->
                    <div v-cloak class="fixed top-0 bottom-0 right-0 translate-x-full bg-white border-gray-200 lg:sticky lg:border-l lg:shrink-0 lg:pt-6 lg:transform-none sm:w-1/2 lg:w-64 lg:z-0 md:w-104 sidebar-right skeleton dark:bg-dark-850 dark:border-dark-650">
                        <div class="pl-5">
                            <div class="w-32 h-3 mb-4 bg-gray-200 dark:bg-dark-600 rounded-full loading"></div>
                            <div class="w-48 h-3 mb-4 bg-gray-200 dark:bg-dark-600 rounded-full loading"></div>
                            <div class="w-40 h-3 mb-4 bg-gray-200 dark:bg-dark-600 rounded-full loading"></div>
                        </div>
                    </div>
                
                    <!-- User should be able to hide sidebar right -->
                    <doc-sidebar-right v-cloak></doc-sidebar-right>
                </div>

            </div>
        </div>
    
        <doc-search-mobile></doc-search-mobile>
        <doc-back-to-top></doc-back-to-top>
    </div>


    <div id="docs-overlay-target"></div>

    <script data-cfasync="false">window.__DOCS__ = { "title": "LoRA", level: 2, icon: "file", hasPrism: true, hasMermaid: false, hasMath: true, tocDepth: 23 }</script>
</body>
</html>
